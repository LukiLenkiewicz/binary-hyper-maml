{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\detac\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import ViTImageProcessor, ViTModel, CLIPImageProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Hypernetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layer_size, chunk_size, chunk_emb_size, num_chunks):\n",
    "        super().__init__()\n",
    "        input_size = input_size + chunk_emb_size\n",
    "        self.chunk_embeddings = self._generate_chunk_embeddings(chunk_emb_size, num_chunks) \n",
    "\n",
    "        hypernet_layers = self._prepare_layers(num_layers, layer_size, input_size, chunk_size)\n",
    "        self.hypernet = nn.Sequential(*hypernet_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fast_weights = []\n",
    "        for chunk_emb in self.chunk_embeddings:\n",
    "            cat_ = torch.cat((x, chunk_emb), dim=1)\n",
    "            fast_weight_chunk = self.hypernet(cat_)\n",
    "            fast_weights.append(fast_weight_chunk)\n",
    "        return fast_weights\n",
    "\n",
    "    def _prepare_layers(self, num_layers, layer_size, input_size, chunk_size):\n",
    "        input_layer = nn.Linear(in_features=input_size, out_features=layer_size)\n",
    "        layers = [input_layer, nn.ReLU()]\n",
    "        for _ in range(num_layers-1):\n",
    "            layer = nn.Linear(layer_size, layer_size)\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Linear(in_features=layer_size, out_features=chunk_size))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        return layers\n",
    "        \n",
    "    def _generate_chunk_embeddings(self, chunk_emb_size, num_chunks):\n",
    "        return [torch.rand((1, chunk_emb_size)) for _ in range(num_chunks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Linear_fw(nn.Linear): #used in MAML to forward input with fast weight\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__(in_features, out_features)\n",
    "        self.weight.fast = self.weight\n",
    "        self.bias.fast = self.bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.weight.fast is not None and self.bias.fast is not None:\n",
    "            out = F.linear(x, self.weight.fast, self.bias.fast) #weight.fast (fast weight) is the temporaily adapted weight\n",
    "        else:\n",
    "            out = F.linear(x, self.weight, self.bias)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_FW(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layer_size, num_classes):\n",
    "        super().__init__()\n",
    "        layers = self._generate_layers(input_size, num_layers, layer_size, num_classes)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def _generate_layers(self, input_size, num_layers, layer_size, num_classes):\n",
    "        layers = [Linear_fw(input_size, layer_size), nn.ReLU()]\n",
    "        for _ in range(num_layers-2):\n",
    "            layers.append(Linear_fw(layer_size, layer_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(Linear_fw(layer_size, num_classes))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP_FW(768, 4, 128, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132101"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in mlp.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embedding = torch.rand((1,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernet = Hypernetwork(768, 4, 500, 128, 20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates = hypernet(input_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "clip_image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "img = Image.open(\"/home/lukasz/binary-hyper-maml/filelists/emnist/emnist/13/11.png\").convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAcABwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDwGON5pUijQvI7BVVRkknoBXtFh8LvDfhTwmnibxtLd3ihlSWxsTjyWbGFc5B3DuMjHvXkOlajJpGq2uowxQyy28gkRJkDoSOmQeten219LqvwA8S3d/cM08muibIX70jBCenQck0AZ/xD8IeHIfD9h4r8FyM+jzv5E8TSF2glwSAc5K8cYJ9PWvNK9MilhT9nKaNLjZI+vgOhX758sHAP0AOfbFeZ0AFeseF47q9+Aeu6fp8Ulxdz6xGnkRIXcgrHjgcjlevtXk9dHofjzxJ4aspLPRtQWyik++YreLe31fbuPXjnigDofHFvL4U8F6J4MuTCb4SvqV4sbbvLdxtVG/2go5/CvO6mu7u4v7uW6u55J7iVi0ksjFmYnuSahoA//9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACzElEQVR4Ab2VzUtiURjG05RKoVy0qjaJCBLhItBVu3ZFe2nrxnahtBP/BEFsXShIhBJt2rQIpI9Ni0BcmZssCErNj1DHcWZ+3DNc7tx7jEvDzFkczn3PeZ7zvB/nvRMT/2BYzHNalcH5H8r4BGiKdHJycmtra2dnx+/3w1UqlW5vbw8ODvr9/ifU8i2LxbK8vJxIJAqFQq/XQ99PZbAYjUbRaNRms8mR46wwoiubzQ4Gg+/K+KYM4Tvz2dmZ2+0eB5fbAbTbbaGOOZVK+Xy+tbU1BFarVZQOh8OTkxPiLMcbrfPz8+FwGBhgIY0gCGeZCW6lUsH+8PDgdDqNcLklHo8jE9/T6bQgPTw8dLlc4jSR8Xg8r6+vXLm3t2ekkIifmppC6cvLy/7+/tHREfEE9vb2puaabLVaLWbYZ2dnTZHu7u5GIpH393e0wEUQgHW7XcEuKFAthOOHKVLilclkQqEQLM/Pz09PT8BICLp0ePSSK52RT0mhXVxcUNuNRoNtfBSYlZUVpKFdfAqvHQ6HekDYxSyJKYVZr9d1p1dXV4W/wHhg6+vrlIHdbuelaenGkhoPYaEYRHBZk/RisShCLC0piVIp6fn5OfFVt2q1WrPZ5O3yrlSjupDEVN3TLlCEv6rYD2VQEmRVe0yszSrd3NxcWlpS8YuLiwsLC5Sdeo26xcIs6R8Yq5W8ka5Op6PLpzhmlrRcLqPrN8ZqpcKgu7u7I2na+75OSiXNzc1Rdjc3N0ZGLGaV0vcgEhRer3djY+P6+vrq6uqvSB8fH0k4Gunc+Xwe93O5HJ3hi6SUPeEjJ4QvEAjQDOnfdKz7+3tpN5FeozfGYjE0Xl5eJpNJChMiapM+bewveuS4bwp+e3ub98OjpCRhhBfGmZmZcRDs+m6mPUp58xcJBoPT09PogvH4+Pj09JRftLQ8VexnpByiwrVuavu0SvGfFr8AmBmfGj2QqfYAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = vit_image_processor(images=img, return_tensors=\"pt\")\n",
    "pixel_values = inputs.pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outptut = model(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outptut[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ViTModel.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = feature_extractor(pixel_values)\n",
    "sequence_output = outputs[0]\n",
    "\n",
    "sequence_output[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
